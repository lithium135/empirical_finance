{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee43cf7-9cc1-4c92-976a-b1924499919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "pd.options.mode.chained_assignment = None  \n",
    "\n",
    "# trim data\n",
    "def get_truncated_df(merge_df, columns, year_column_name=None, low=0.01, high=0.99):\n",
    "    merge_df_copy = merge_df.copy()\n",
    "    if year_column_name == None:\n",
    "        for column in columns: \n",
    "            _1pct, _99pct = merge_df_copy[column].quantile(q=low), merge_df_copy[column].quantile(q=high)\n",
    "            merge_df_copy[column].where((merge_df_copy[column] < _99pct) & (merge_df_copy[column] > _1pct), math.nan, inplace=True)\n",
    "    else: \n",
    "        merge_df_list = []\n",
    "        merge_year_df_list = [[year, merge_year_df] for year, merge_year_df in merge_df_copy.groupby(year_column_name)]\n",
    "        for year, merge_year_df in tqdm(merge_year_df_list):\n",
    "            for column in columns: \n",
    "                _1pct, _99pct = merge_year_df[column].quantile(q=low), merge_year_df[column].quantile(q=high)\n",
    "                merge_year_df[column].where(~((merge_year_df[column] > _99pct) | (merge_year_df[column] < _1pct)), math.nan, inplace=True)\n",
    "            merge_df_list.append(merge_year_df)\n",
    "        merge_df_copy = pd.concat(merge_df_list)\n",
    "    \n",
    "    return merge_df_copy\n",
    "\n",
    "def get_winsorized_df(merge_df, columns, year_column_name=None, low=0.01, high=0.99):\n",
    "    merge_df_copy = merge_df.copy()\n",
    "    if year_column_name == None:\n",
    "        for column in columns: \n",
    "            _1pct, _99pct = merge_df_copy[column].quantile(q=low), merge_df_copy[column].quantile(q=high)\n",
    "            merge_df_copy[column].where((merge_df_copy[column] < _99pct), _99pct, inplace=True)\n",
    "            merge_df_copy[column].where((merge_df_copy[column] > _1pct), _1pct, inplace=True)\n",
    "    else: \n",
    "        merge_df_list = []\n",
    "        merge_year_df_list = [[year, merge_year_df] for year, merge_year_df in merge_df_copy.groupby(year_column_name)]\n",
    "        for year, merge_year_df in tqdm(merge_year_df_list):\n",
    "            for column in columns: \n",
    "                low_val, high_val = merge_year_df[column].quantile(q=low), merge_year_df[column].quantile(q=high)\n",
    "                merge_year_df[column].where(~(merge_year_df[column] > high_val), high_val, inplace=True)\n",
    "                merge_year_df[column].where(~(merge_year_df[column] < low_val), low_val, inplace=True)\n",
    "            merge_df_list.append(merge_year_df)\n",
    "        merge_df_copy = pd.concat(merge_df_list)\n",
    "    return merge_df_copy\n",
    "\n",
    "def get_cal_qtr(pmon):\n",
    "    if pmon in [1, 2, 3]: \n",
    "        return 1 \n",
    "    elif pmon in [4, 5, 6]: \n",
    "        return 2\n",
    "    elif pmon in [7, 8, 9]: \n",
    "        return 3\n",
    "    else: \n",
    "        return 4\n",
    "    \n",
    "def winsorize_strict(x, low, high): \n",
    "    if x < low: \n",
    "        return low \n",
    "    elif x > high: \n",
    "        return high\n",
    "    else: \n",
    "        return x\n",
    "\n",
    "def trim_strict(x, low, high): \n",
    "    if x < low: \n",
    "        return math.nan \n",
    "    elif x > high: \n",
    "        return math.nan\n",
    "    else: \n",
    "        return x\n",
    "    \n",
    "def get_2d_sic(x): \n",
    "    try: \n",
    "        return float(str(x)[:2])\n",
    "    except: \n",
    "        return math.nan\n",
    "    \n",
    "def trim_bouchaud(col, df):\n",
    "    sue_prc_stats = df[col].describe()\n",
    "    iqr, med = (sue_prc_stats.loc['75%'] - sue_prc_stats.loc['25%']), sue_prc_stats.loc['50%']\n",
    "    delta = 5 * iqr\n",
    "    return df[col].apply(lambda x: trim_strict(x, low=med-delta, high=med+delta))\n",
    "\n",
    "def trim_bouchaud_cols(cols, df):\n",
    "    df = df.copy(deep=True)\n",
    "    for col in tqdm(cols):\n",
    "        df[col] = trim_bouchaud(col, df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d7201-fc0c-4dda-a8db-2bab3518c2d2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7223e1c1-7cab-449c-811e-d5a5a67a39a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## read cleaned IBES file\n",
    "sue_df = pd.read_pickle('data/checkpoint_data/sue_df_linked.pkl')\n",
    "sue_df.columns = ['IBES_TICKER', 'OFTIC', 'MEASURE', 'FISCALP', 'PYEAR', 'PMON', 'USFIRM',\n",
    "                 'ibes_anndate', 'actual', 'surpmean', 'surpstdev', 'suescore', 'PERMNO',\n",
    "                 'pyear_month_date', 'bias', 'date', 'SHRCD', 'SICCD', 'NUMEST_sue']\n",
    "sue_df['2d_sic'] = sue_df['SICCD'].apply(lambda x: get_2d_sic(x))\n",
    "sue_df = sue_df.sort_values(['pyear_month_date']).drop_duplicates(['PERMNO', 'ibes_anndate'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4674f6d5-18cb-4475-bed6-c88e088a652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read cleaned returns file\n",
    "ret_df = pd.read_pickle('data/checkpoint_data/crsp_dailydata_holding_w_period_returns.pkl')\n",
    "ret_df[['RET', '30d_ret', '60d_ret', '2d_ret', '3d_ret','back_1d', 'back_5d', 'back_30d']] = 100 * ret_df[['RET', '30d_ret', '60d_ret', '2d_ret', '3d_ret','back_1d', 'back_5d', 'back_30d']]\n",
    "ret_df.columns = ['ret_date', 'PERMNO', 'TICKER', 'PRC', 'RET', '60d_ret', '2d_ret', \n",
    "                  '3d_ret', 'back_1d_ret', 'back_5d_ret', 'back_30d_ret', 'lagged_PRC', '30d_ret']\n",
    "ret_df['back_30d_to_60d_ret'] = ((1 + ret_df['back_30d_ret']/100) * (1 + ret_df['60d_ret']/100) - 1) * 100\n",
    "ret_df['back_30d_to_30d_ret'] = ((1 + ret_df['back_30d_ret']/100) * (1 + ret_df['30d_ret']/100) - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1c5d20-2fda-4102-90b7-0019748aaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in 10yr yield data\n",
    "tenyr_df = pd.read_csv('data/ten_yr_yields.csv', parse_dates=['DATE'])\n",
    "tenyr_df['DGS10'] = tenyr_df['DGS10'].apply(lambda x: math.nan if x == '.' else float(x))\n",
    "tenyr_df = tenyr_df.dropna()\n",
    "tenyr_df.columns = ['tenyr_date', '10yT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101a6354-9788-4975-8888-47232f321b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in 1m yield data\n",
    "ff3_df = pd.read_csv('C:/Users/jackl/OneDrive/Documents/finance_research/sunderam/data/fama_french_3factor_copy.csv')\n",
    "ff3_df['date'] = ff3_df['date'].apply(lambda x: pd.to_datetime(str(x)+'01', yearfirst=True))\n",
    "ff3_df['month'], ff3_df['year'] = ff3_df['date'].dt.month, ff3_df['date'].dt.year\n",
    "ff3_df = ff3_df[['Mkt-RF', 'RF', 'month', 'year']]\n",
    "ff3_df.columns = ['Mkt-RF', '1mT', 'month_1m', 'year_1m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e1d728-d927-4fbe-99fc-1896d5308d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in fundamental data for M/B\n",
    "an_df = pd.read_csv('data/price_to_book.csv', parse_dates=['datadate'])\n",
    "an_df = an_df[['GVKEY', 'cusip', 'LPERMNO', 'datadate', 'fyear', 'ceq', 'txditc', 'prcc_f', 'csho']]\n",
    "an_df['be'] = (an_df['ceq'] + an_df['txditc']).apply(lambda x: math.nan if x < 0 else x)\n",
    "an_df['me'] = an_df['prcc_f'] * an_df['csho']\n",
    "an_df['mb'] = an_df['me']/an_df['be']\n",
    "an_df['bm'] = an_df['be']/an_df['me']\n",
    "an_df['LPERMNO'] = an_df['LPERMNO'].astype('Int64')\n",
    "an_df['8dcusip'] = an_df['cusip'].apply(lambda x: x[:-1])\n",
    "an_df = an_df.sort_values(['datadate']).drop_duplicates(['LPERMNO', 'fyear'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0cd7dc-167a-442b-8a0b-5cf1d017973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltg_df = pd.read_pickle('data/checkpoint_data/ltg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9a414-00eb-4ad5-ab78-e481b65dd465",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b01556-52aa-4cb8-9765-ff60cf80cadd",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 329. MiB for an array with shape (1, 43065497) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cd65fe27bea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## merge IBES file with returns data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m full1_df = pd.merge(sue_df, \n\u001b[0m\u001b[0;32m      3\u001b[0m                     \u001b[0mret_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ibes_anndate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PERMNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ret_date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PERMNO'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 74\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mleft_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_labels_or_levels\u001b[1;34m(self, keys, axis)\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;31m# This ensures that we always perform exactly one copy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m         \u001b[1;31m# ``copy`` and/or ``inplace`` options could be added in the future.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1754\u001b[1;33m         \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5993\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5994\u001b[0m         \"\"\"\n\u001b[1;32m-> 5995\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5996\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    822\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 329. MiB for an array with shape (1, 43065497) and data type float64"
     ]
    }
   ],
   "source": [
    "## merge IBES file with returns data\n",
    "full1_df = pd.merge(sue_df, \n",
    "                    ret_df, \n",
    "                    left_on=['ibes_anndate', 'PERMNO'], \n",
    "                    right_on=['ret_date', 'PERMNO'], \n",
    "                    how='left')\n",
    "\n",
    "del sue_df\n",
    "del ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa490f7-a287-4ae4-9840-4d0e7b678c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define dE/P\n",
    "full1_df['de_P'] = (full1_df['actual'] - full1_df['surpmean'])/full1_df['lagged_PRC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e3f1d-c02c-4660-9975-ac777972888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get quarter and month indices\n",
    "full1_df['CAL_QTR'] = full1_df['PMON'].apply(lambda x: get_cal_qtr(x))\n",
    "full1_df['anndats_month'] = full1_df['ibes_anndate'].dt.month\n",
    "full1_df['anndats_year'] = full1_df['ibes_anndate'].dt.year\n",
    "full1_df['anndats_CAL_QTR'] = full1_df['anndats_month'].apply(lambda x: get_cal_qtr(x))\n",
    "\n",
    "#get qtr index for the quarter the announcement date occurs\n",
    "anndats_year_qtrs_tuples = list(tuple(zip(list(full1_df['anndats_year']), list(full1_df['anndats_CAL_QTR']))))\n",
    "dict_year_qtrs_mapping = dict(zip(sorted(set(anndats_year_qtrs_tuples), key=lambda element: (element[0], element[1])), range(len(set(anndats_year_qtrs_tuples)))))\n",
    "full1_df['qtr_index'] = [dict_year_qtrs_mapping[tuple_] for tuple_ in anndats_year_qtrs_tuples]\n",
    "\n",
    "#get qtr index for the quarter the announcement is for\n",
    "anndats_year_qtrs_tuples = list(tuple(zip(list(full1_df['PYEAR']), list(full1_df['CAL_QTR']))))\n",
    "dict_year_qtrs_mapping = dict(zip(sorted(set(anndats_year_qtrs_tuples), key=lambda element: (element[0], element[1])), range(len(set(anndats_year_qtrs_tuples)))))\n",
    "full1_df['actual_qtr_index'] = [dict_year_qtrs_mapping[tuple_] for tuple_ in anndats_year_qtrs_tuples]\n",
    "\n",
    "#get month index for the announcement date\n",
    "anndats_year_month_tuples = list(tuple(zip(list(full1_df['anndats_year']), list(full1_df['anndats_month']))))\n",
    "dict_year_month_mapping = dict(zip(sorted(set(anndats_year_month_tuples), key=lambda element: (element[0], element[1])), range(len(set(anndats_year_month_tuples)))))\n",
    "full1_df['month_index'] = [dict_year_month_mapping[tuple_] for tuple_ in anndats_year_month_tuples]\n",
    "\n",
    "#get four-year interval index\n",
    "full1_df['interval_index'] = full1_df['anndats_year'].apply(lambda x: math.floor((x - 1992)/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42af53b-bc9b-4c8f-bb67-8eebbc22bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with LTG data\n",
    "full1_df = pd.merge(full1_df, \n",
    "                    ltg_df, \n",
    "                    left_on=['IBES_TICKER', 'anndats_month', 'anndats_year'], \n",
    "                    right_on=['TICKER', 'STATPERS_month', 'STATPERS_year'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0256f2-f3fd-4dea-8142-7ddcb66954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with interest rate data\n",
    "full1_df = pd.merge(full1_df, \n",
    "                    ff3_df, \n",
    "                    left_on=['anndats_month', 'anndats_year'], \n",
    "                    right_on=['month_1m', 'year_1m'])\n",
    "\n",
    "full1_df = pd.merge_asof(full1_df.sort_values('ibes_anndate'), \n",
    "                         tenyr_df, \n",
    "                         left_on=['ibes_anndate'], \n",
    "                         right_on=['tenyr_date'], \n",
    "                         direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500dda3-4f5c-40ac-b53e-2fe9f3a0765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with M/B data\n",
    "full1_df = pd.merge_asof(full1_df.sort_values('ibes_anndate'), \n",
    "                          an_df.sort_values('datadate'), \n",
    "                          left_by='PERMNO', \n",
    "                          right_by='LPERMNO', \n",
    "                          left_on='ibes_anndate', \n",
    "                          right_on='datadate', \n",
    "                          direction='backward', \n",
    "                          tolerance=pd.Timedelta(days=365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00833b6-4c3b-4cab-8d44-70621d5bcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up earnings data\n",
    "full1_df['log_earnings'] = np.log(full1_df['actual'] * full1_df['csho'])\n",
    "full1_df['actual_qtr_index_match_1'] = full1_df['actual_qtr_index'] - 1\n",
    "full1_df['actual_qtr_index_match_4'] = full1_df['actual_qtr_index'] - 4\n",
    "earnings_df = full1_df[['PERMNO', 'actual_qtr_index', 'log_earnings']]\n",
    "earnings_df.columns = ['PERMNO', 'match_index', 'log_earnings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15a360-023c-4e35-84ac-acd0ff96f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge raw earnings data\n",
    "full1_df = pd.merge(full1_df,  \n",
    "                    earnings_df, \n",
    "                    left_on=['PERMNO', 'actual_qtr_index_match_1'], \n",
    "                    right_on=['PERMNO', 'match_index'], \n",
    "                    how='left', \n",
    "                    suffixes=['', '_lag1']).drop(columns=['match_index', 'actual_qtr_index_match_1'])\n",
    "full1_df['diff_log_earnings'] = full1_df['log_earnings'] - full1_df['log_earnings_lag1']\n",
    "\n",
    "earnings_df = full1_df[['PERMNO', 'actual_qtr_index', 'log_earnings', 'diff_log_earnings']]\n",
    "earnings_df.columns = ['PERMNO', 'match_index', 'log_earnings', 'diff_log_earnings']\n",
    "full1_df = pd.merge(full1_df,  \n",
    "                    earnings_df, \n",
    "                    left_on=['PERMNO', 'actual_qtr_index_match_4'], \n",
    "                    right_on=['PERMNO', 'match_index'], \n",
    "                    how='left', \n",
    "                    suffixes=['', '_lag4']).drop(columns=['match_index', 'actual_qtr_index_match_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce713904-0f46-4bba-b6cc-02eaeac2fb19",
   "metadata": {},
   "source": [
    "## Trim Data and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35078ae-6db5-4c59-ae15-a7d134ff87d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## trimmed data starts here\n",
    "full1_df_trimmed = full1_df.copy()\n",
    "\n",
    "#replace infs with nans\n",
    "full1_df_trimmed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "#drop duplicates where a company has more than one observation for the same quarter being predicted\n",
    "full1_df_trimmed = full1_df_trimmed.drop_duplicates(subset=['pyear_month_date', 'PERMNO'])\n",
    "\n",
    "percentile_trim_vars = ['30d_ret',\n",
    "                        '60d_ret', \n",
    "                        '2d_ret', \n",
    "                        'back_30d_to_60d_ret',\n",
    "                        'back_30d_to_30d_ret',\n",
    "                        'back_1d_ret', \n",
    "                        'back_5d_ret', \n",
    "                        'back_30d_ret', \n",
    "                        'mb', \n",
    "                        'bm', \n",
    "                        'log_earnings', \n",
    "                        'log_earnings_lag4', \n",
    "                        'diff_log_earnings', \n",
    "                        'diff_log_earnings_lag4'\n",
    "] \n",
    "\n",
    "bouchaud_cols = ['suescore', \n",
    "                'de_P', \n",
    "                'MEDEST', \n",
    "                'MEANEST',\n",
    "                'MEDEST_p1', \n",
    "                'MEANEST_p1',\n",
    "                'MEDEST_p2', \n",
    "                'MEANEST_p2', \n",
    "                'MEDEST_l1', \n",
    "                'MEANEST_l1', \n",
    "                'chg_MEDEST_2', \n",
    "                'chg_MEANEST_2', \n",
    "                'chg_MEDEST_3', \n",
    "                'chg_MEANEST_3', \n",
    "                'chg_MEANEST_rev31',\n",
    "                'chg_MEDEST_rev31',\n",
    "                'chg_MEANEST_rev32',\n",
    "                'chg_MEDEST_rev32'\n",
    "]\n",
    "\n",
    "#trim 1%/99% conditional on quarter\n",
    "full1_df_trimmed = get_truncated_df(full1_df_trimmed, \n",
    "                                    percentile_trim_vars,\n",
    "                                    year_column_name='qtr_index', \n",
    "                                    low=0.01, \n",
    "                                    high=0.99)\n",
    "\n",
    "full1_df_trimmed = trim_bouchaud_cols(bouchaud_cols, \n",
    "                                      full1_df_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c39bc-467d-4c32-a67f-3cbdc3b7c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary Stats\n",
    "sum_stats_vars = percentile_trim_vars + bouchaud_cols\n",
    "sum_stats = full1_df_trimmed[sum_stats_vars].describe(percentiles=[.25, .5, .75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756b9bf-d8f3-410b-aacf-8d6097c09c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_match = pd.read_csv('C:/Users/jackl/OneDrive/Documents/finance_research/ec980/seo_data/raw_data/SIC_to_Fama_French_industry.csv')\n",
    "full1_df_trimmed = pd.merge(full1_df_trimmed, \n",
    "                            ff_match, \n",
    "                            left_on='SICCD', \n",
    "                            right_on='SIC', \n",
    "                            how='left'\n",
    ").drop(columns=['SIC', 'SIC0'])\n",
    "\n",
    "#get deciles for various variables\n",
    "panel_df = full1_df_trimmed.copy()\n",
    "ff_12_list = list(set(panel_df['FF_12'].dropna()))\n",
    "total_dfs = []\n",
    "for ff_12 in tqdm(ff_12_list): \n",
    "    panel_subset_df = panel_df[panel_df['FF_12']==ff_12]\n",
    "    for qtr_index in range(panel_df['qtr_index'].max() + 1):\n",
    "        panel_subset_df_qtr = panel_subset_df[panel_subset_df['qtr_index']==qtr_index]\n",
    "        panel_subset_df_qtr['suescore_decile'] = pd.qcut(panel_subset_df_qtr['suescore'].rank(method='first'), 10, labels=False, duplicates='drop') + 1\n",
    "        panel_subset_df_qtr['de_P_decile'] = pd.qcut(panel_subset_df_qtr['de_P'].rank(method='first'), 10, labels=False, duplicates='drop') + 1\n",
    "        panel_subset_df_qtr['mb_decile'] = pd.qcut(panel_subset_df_qtr['mb'].rank(method='first'), 10, labels=False, duplicates='drop') + 1\n",
    "        panel_subset_df_qtr['mb_indicator'] = pd.qcut(panel_subset_df_qtr['mb'].rank(method='first'), 2, labels=False, duplicates='drop')\n",
    "        total_dfs.append(panel_subset_df_qtr)\n",
    "full1_df_trimmed = pd.concat(total_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d447d62-80d1-4f5d-ad93-2154c2b49b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full1_df_trimmed[['PERMNO', 'LPERMNO']] = full1_df_trimmed[['PERMNO', 'LPERMNO']].astype(float)\n",
    "full1_df_trimmed.to_stata('data/checkpoint_data/sue_ret_df_trimmed_year_cond.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a25ef-2ba7-4beb-83be-457f12c3a74c",
   "metadata": {},
   "source": [
    "## Create M/B on LTG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "572e3dc2-33fc-43e1-ac50-4d965c8d9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in fundamental data for M/B\n",
    "an_df = pd.read_csv('data/price_to_book.csv', parse_dates=['datadate'])\n",
    "an_df = an_df[['GVKEY', 'cusip', 'LPERMNO', 'datadate', 'fyear', 'ceq', 'txditc', 'prcc_f', 'csho']]\n",
    "an_df['be'] = (an_df['ceq'] + an_df['txditc']).apply(lambda x: math.nan if x < 0 else x)\n",
    "an_df['me'] = an_df['prcc_f'] * an_df['csho']\n",
    "an_df['mb'] = an_df['me']/an_df['be']\n",
    "an_df['bm'] = an_df['be']/an_df['me']\n",
    "an_df['LPERMNO'] = an_df['LPERMNO'].astype('Int64')\n",
    "an_df['8dcusip'] = an_df['cusip'].apply(lambda x: x[:-1])\n",
    "an_df = an_df.sort_values(['datadate']).drop_duplicates(['LPERMNO', 'fyear'], keep='last')\n",
    "\n",
    "ltg_df = pd.read_pickle('data/checkpoint_data/ltg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ca1584-7998-464b-877a-3adf7d63c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltg_mb_df = pd.merge_asof(an_df.sort_values('datadate'), \n",
    "                          ltg_df.sort_values('STATPERS'), \n",
    "                          left_by='8dcusip', \n",
    "                          right_by='CUSIP', \n",
    "                          left_on='datadate', \n",
    "                          right_on='STATPERS', \n",
    "                          direction='backward', \n",
    "                          tolerance=pd.Timedelta(days=31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82aacc3e-57da-4a62-adfe-3bc7e9038df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltg_mb_df = ltg_mb_df.dropna(subset=['fyear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84507449-862d-4f84-8d4a-b2e16989b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltg_mb_df['interval_index'] = ltg_mb_df['fyear'].apply(lambda x: math.floor((x - 1992)/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a5486d1-2ee1-4d88-9bf6-6a7ef7de03db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:02<00:00, 10.22it/s]\n"
     ]
    }
   ],
   "source": [
    "ltg_mb_df = get_truncated_df(ltg_mb_df, \n",
    "                             ['MEDEST', 'MEANEST','MEDEST_p1', 'MEANEST_p1','MEDEST_p2', \n",
    "                              'MEANEST_p2', 'MEDEST_l1', 'MEANEST_l1', 'chg_MEDEST_2', \n",
    "                              'chg_MEANEST_2', 'chg_MEDEST_3', 'chg_MEANEST_3', 'mb', 'bm'], \n",
    "                              year_column_name='fyear', \n",
    "                              low=0.01, \n",
    "                              high=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7004fe4e-a6a0-44da-a826-f5a82eadfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltg_mb_df['mb_trimmed_strict'] = ltg_mb_df['mb'].apply(lambda x: trim_strict(x, -math.inf, 10))\n",
    "ltg_mb_df['MEANEST_trimmed_strict'] = ltg_mb_df['MEANEST'].apply(lambda x: trim_strict(x, -30, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "810ecbfc-0f81-4d27-84ac-4c3f55eb415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jackl\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\stata.py:2346: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    8dcusip   ->   _8dcusip\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  warnings.warn(ws, InvalidColumnName)\n"
     ]
    }
   ],
   "source": [
    "ltg_mb_df[['LPERMNO']] = ltg_mb_df[['LPERMNO']].astype(float)\n",
    "ltg_mb_df.to_stata('data/checkpoint_data/mb_ltg.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0af72-dfe7-4530-9b6e-47172f2c07ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
